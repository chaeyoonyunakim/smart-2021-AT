{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMART2021_AT Prediction Task_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOC3P/woPOJgBdjZ9TtZX7z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaeyoonyunakim/smart-2021-AT_Answer_Type_Prediction/blob/main/SMART2021_AT_Prediction_Task_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1YCTaqmYYfV"
      },
      "source": [
        "## 0. Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMErWmmfkxqp",
        "outputId": "7f895f07-173a-4e3b-bef7-a0bb6222d47a"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "\n",
        "# authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# locate dataset folder\n",
        "%ls -l '/content/drive/My Drive/2021_INM363_SMART/task1_dbpedia_train.json'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "-rw------- 1 root root 9263124 Sep 29 08:07 '/content/drive/My Drive/2021_INM363_SMART/task1_dbpedia_train.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqGDQCAWfPtF"
      },
      "source": [
        "# set working directory\n",
        "import os\n",
        "os.chdir(path = \"/content/drive/My Drive/2021_INM363_SMART/\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1KBaWJJk4Qg"
      },
      "source": [
        "# import basics\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "seed = 20211001\n",
        "import regex as re\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qghbo96ZlCTN",
        "outputId": "83c0a0bf-8539-44a8-e1aa-80d858dc0526"
      },
      "source": [
        "# import nlp relevants\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "set(stopwords.words('english'))\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# for bag-of-words (bow)\n",
        "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGJtj4fPlFid"
      },
      "source": [
        "# import scikit-learn tools for modelling and evaluation\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApUkA-SzlD3c"
      },
      "source": [
        "# import algos\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUs8fZVzZnq2"
      },
      "source": [
        "## 1. Data Loading & Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRL8r9it-bla"
      },
      "source": [
        "***Filter ''o'' in question out***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1W4YPu2wBEo"
      },
      "source": [
        "ap_re = re.compile(\"(^\\')(.*)(\\'$)\")\n",
        "\n",
        "def remove_appostrophe(x):\n",
        "  matches = ap_re.findall(x)\n",
        "  if len(matches) == 1:\n",
        "    return matches[0][1]\n",
        "  else:\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7bJ0m9W-XH7"
      },
      "source": [
        "***Load Dataset*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWVZEsmFwBLW"
      },
      "source": [
        "def load_data(path, train = True):\n",
        "  data = pd.read_json(path)\n",
        "  \n",
        "  # map na strings to nan\n",
        "  data.loc[:, \"question\"].replace(\"n/a\", np.nan, inplace = True)\n",
        "  \n",
        "  # drop na in data\n",
        "  if train:\n",
        "    data.dropna(subset=['id', 'question', 'category'], inplace=True)\n",
        "  else:\n",
        "    data.dropna(subset=['id', 'question'], inplace=True)\n",
        "\n",
        "  # remove apostrophes from the start and end of str\n",
        "  data.loc[:, \"question\"] = data[\"question\"].map(lambda x: remove_appostrophe(x))\n",
        "\n",
        "  # for the training data remove rows that have no types\n",
        "  if train:\n",
        "    data = data[data[\"type\"].map(lambda x : len(x) != 0)]\n",
        "\n",
        "  return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw9SNMabwBRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "875530ed-00eb-4a3f-c1dd-9fa54ef08e09"
      },
      "source": [
        "# load dataset\n",
        "t0 = time.time()\n",
        "train_data2020 = load_data('/content/drive/My Drive/2021_INM363_SMART/smarttask_dbpedia_train.json')\n",
        "test_data2020 = load_data('/content/drive/My Drive/2021_INM363_SMART/smarttask_dbpedia_test.json', train = False)\n",
        "\n",
        "train_data2021 = load_data('/content/drive/My Drive/2021_INM363_SMART/task1_dbpedia_train.json')\n",
        "test_data2021 = load_data('/content/drive/My Drive/2021_INM363_SMART/task1_dbpedia_test.json', train = False)\n",
        "t1 = time.time()-t0\n",
        "print(t1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.4682600498199463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t3x_V5ogknH",
        "outputId": "bb65f8f4-ad58-48b0-9c1a-30ec7e87bf45"
      },
      "source": [
        "# check data size\n",
        "train_data2020.shape, test_data2020.shape, train_data2021.shape, test_data2021.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17482, 4), (4378, 4), (36670, 4), (9104, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NRe1d08Rmo3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "232d5815-bf35-4f5a-bf87-f9f0f97f7b22"
      },
      "source": [
        "train_data2020"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dbpedia_1177</td>\n",
              "      <td>Was Jacqueline Kennedy Onassis a follower of M...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dbpedia_14427</td>\n",
              "      <td>What is the name of the opera based on Twelfth...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:Opera, dbo:MusicalWork, dbo:Work]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dbpedia_16615</td>\n",
              "      <td>When did Lena Horne receive the Grammy Award f...</td>\n",
              "      <td>literal</td>\n",
              "      <td>[date]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dbpedia_23480</td>\n",
              "      <td>Do Prince Harry and Prince William have the sa...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dbpedia_3681</td>\n",
              "      <td>What is the subsidiary company working for Leo...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:EducationalInstitution, dbo:Organisation,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17566</th>\n",
              "      <td>dbpedia_7462</td>\n",
              "      <td>Is the flexural strain at break of the acrylon...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17567</th>\n",
              "      <td>dbpedia_17610</td>\n",
              "      <td>Where did Hilary Putnam receive their Ph.D.?</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:University, dbo:EducationalInstitution, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17568</th>\n",
              "      <td>dbpedia_505</td>\n",
              "      <td>Who replaced Charles Evans Hughes as the Chief...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:Person, dbo:Agent]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17569</th>\n",
              "      <td>dbpedia_18989</td>\n",
              "      <td>Name the river with source as Columbia Lake an...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:River, dbo:Stream, dbo:BodyOfWater, dbo:N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17570</th>\n",
              "      <td>dbpedia_11517</td>\n",
              "      <td>When will Selma Lagerlöf start their membershi...</td>\n",
              "      <td>literal</td>\n",
              "      <td>[date]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17482 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  ...                                               type\n",
              "0       dbpedia_1177  ...                                          [boolean]\n",
              "1      dbpedia_14427  ...             [dbo:Opera, dbo:MusicalWork, dbo:Work]\n",
              "2      dbpedia_16615  ...                                             [date]\n",
              "3      dbpedia_23480  ...                                          [boolean]\n",
              "4       dbpedia_3681  ...  [dbo:EducationalInstitution, dbo:Organisation,...\n",
              "...              ...  ...                                                ...\n",
              "17566   dbpedia_7462  ...                                          [boolean]\n",
              "17567  dbpedia_17610  ...  [dbo:University, dbo:EducationalInstitution, d...\n",
              "17568    dbpedia_505  ...                            [dbo:Person, dbo:Agent]\n",
              "17569  dbpedia_18989  ...  [dbo:River, dbo:Stream, dbo:BodyOfWater, dbo:N...\n",
              "17570  dbpedia_11517  ...                                             [date]\n",
              "\n",
              "[17482 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsgzXPEz-2ry"
      },
      "source": [
        "***Merge two training dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fzeOQLKrifd"
      },
      "source": [
        "# unify the id format\n",
        "train_data2020.loc[:, \"id\"] = train_data2020[\"id\"].map(lambda x: x.split('_')[1])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_dmL_aNrimT"
      },
      "source": [
        "# concat two dataset\n",
        "all_data = pd.concat([train_data2020, train_data2021], axis = 0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzfd0-sSsAAk"
      },
      "source": [
        "# pre processing to remove duplicates in type\n",
        "# transform array to string\n",
        "all_data.loc[:, \"type_str\"] = all_data[\"type\"].map(lambda x: \",\".join(x))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQXhkU89_EVf"
      },
      "source": [
        "# remove duplicates when question, category, and type are same\n",
        "merged_data = all_data.drop_duplicates(subset=[\"question\", \"category\", \"type_str\"])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0khrcFZZqsS"
      },
      "source": [
        "***Tabular representation of the dataset***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "mknRYgOTripL",
        "outputId": "3d55f693-4e3b-4923-f3ec-c13ea4c3256a"
      },
      "source": [
        "merged_data.head(3)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "      <th>type_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1177</td>\n",
              "      <td>Was Jacqueline Kennedy Onassis a follower of M...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "      <td>boolean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14427</td>\n",
              "      <td>What is the name of the opera based on Twelfth...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:Opera, dbo:MusicalWork, dbo:Work]</td>\n",
              "      <td>dbo:Opera,dbo:MusicalWork,dbo:Work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16615</td>\n",
              "      <td>When did Lena Horne receive the Grammy Award f...</td>\n",
              "      <td>literal</td>\n",
              "      <td>[date]</td>\n",
              "      <td>date</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                            type_str\n",
              "0   1177  ...                             boolean\n",
              "1  14427  ...  dbo:Opera,dbo:MusicalWork,dbo:Work\n",
              "2  16615  ...                                date\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "kvZtVjKP6eC6",
        "outputId": "86a988cc-8423-4387-ee2c-8a43ea2232b8"
      },
      "source": [
        "merged_data = merged_data.drop(['type_str'], axis =1)\n",
        "merged_data.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1177</td>\n",
              "      <td>Was Jacqueline Kennedy Onassis a follower of M...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14427</td>\n",
              "      <td>What is the name of the opera based on Twelfth...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:Opera, dbo:MusicalWork, dbo:Work]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16615</td>\n",
              "      <td>When did Lena Horne receive the Grammy Award f...</td>\n",
              "      <td>literal</td>\n",
              "      <td>[date]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23480</td>\n",
              "      <td>Do Prince Harry and Prince William have the sa...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3681</td>\n",
              "      <td>What is the subsidiary company working for Leo...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:EducationalInstitution, dbo:Organisation,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ...                                               type\n",
              "0   1177  ...                                          [boolean]\n",
              "1  14427  ...             [dbo:Opera, dbo:MusicalWork, dbo:Work]\n",
              "2  16615  ...                                             [date]\n",
              "3  23480  ...                                          [boolean]\n",
              "4   3681  ...  [dbo:EducationalInstitution, dbo:Organisation,...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQKlBsKeMkK9",
        "outputId": "1dc13c78-239e-4ac3-d48b-cc3cf6654b14"
      },
      "source": [
        "# check data size\n",
        "all_data.shape, merged_data.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((54152, 5), (39556, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3FaIg_tHHC"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7LfNufdhaCV"
      },
      "source": [
        "***Extract, Transform, Load (ETL)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epe56ypal-qk"
      },
      "source": [
        "class ETL:\n",
        "\n",
        "###############################################################################\n",
        "#   Global Variables     \n",
        "###############################################################################\n",
        "\n",
        "  # text normalization - stemming, lemmatization, stopwords\n",
        "  ps = PorterStemmer()\n",
        "  wordnet_lemmatizer = WordNetLemmatizer() \n",
        "  lst_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "  # remove wh-terms from nltk stopwords\n",
        "  # which is orginally exclusive of 'whose' - so remove other 7 wh-terms only\n",
        "  wh_list = ['who', 'what', 'when', 'where', 'which', 'whom', 'why', 'how']\n",
        "\n",
        "  for ele in wh_list:\n",
        "    lst_stopwords.remove(ele)\n",
        "\n",
        "  # set default feature_extraction parameters\n",
        "  count_vectorizer = None \n",
        "  inv_count_vectorizer_vocab = None\n",
        "  tfidf_vectorizer = None\n",
        "  inv_tfidf_vectorizer_vocab = None\n",
        "\n",
        "  # category maps\n",
        "  category_map = {\"boolean\": 0, \"resource\": 1, \"literal\": 2}\n",
        "  inv_category_map = {}\n",
        "\n",
        "  for label, ind in category_map.items():\n",
        "    inv_category_map[ind] = label\n",
        "\n",
        "  # literal maps\n",
        "  literal_map = {\"date\": 0, \"string\": 1, \"number\": 2}\n",
        "  inv_literal_map = {}\n",
        "\n",
        "  for label, ind in literal_map.items():\n",
        "    inv_literal_map[ind] = label  \n",
        "\n",
        "  # resource maps\n",
        "  type_maps = {}\n",
        "  invtype_maps = {}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#   Main\n",
        "###############################################################################\n",
        "\n",
        "  def __init__(self, path_to_type_maps = None, path_to_vectorizers = None):\n",
        "\n",
        "    # load type maps if requested\n",
        "    if path_to_type_maps != None:\n",
        "      base_dir = path_to_type_maps\n",
        "      paths = [fp for fp in os.listdir(base_dir) if \"type\" in fp]\n",
        "\n",
        "      for fp in paths:\n",
        "        with open(os.path.join(base_dir, fp), \"r\") as input_file:\n",
        "          type_name = fp.split(\"_\")[0]\n",
        "          self.invtype_maps[type_name] = {}\n",
        "          self.type_maps[type_name] = json.load(input_file)[type_name]\n",
        "          for ontology, ind in self.type_maps[type_name].items():\n",
        "            self.invtype_maps[type_name][ind] = ontology\n",
        "\n",
        "    # load data vectorizers if requested\n",
        "    if path_to_vectorizers != None:\n",
        "      base_dir = path_to_vectorizers\n",
        "      paths = [fp for fp in os.listdir(base_dir) if \"vectorizer\" in fp]\n",
        "\n",
        "      for fp in paths:\n",
        "        vectorizer_name = fp.split(\"_\")[0]\n",
        "        with open(os.path.join(base_dir, fp), \"rb\") as input_file:\n",
        "          if vectorizer_name == \"count\":\n",
        "            self.count_vectorizer = pickle.load(input_file)\n",
        "          elif vectorizer_name == \"tfidf\":\n",
        "            self.tfidf_vectorizer = pickle.load(input_file)\n",
        "          else:\n",
        "            NotImplementedError\n",
        "\n",
        "\n",
        "  # split training dataset to exclude validation dataset\n",
        "  # set train:val = 8:2\n",
        "  def split_data(self, data, val_size = 0.2):\n",
        "    df_train, df_test = model_selection.train_test_split(data, test_size = val_size, random_state = seed)\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "  # normalization of question sentences\n",
        "  def _norm_sent(self, sent, rm_stopwords = True, stemming = True, lemmatization = True):\n",
        "    # tokenize - convert from string to list\n",
        "    words = word_tokenize(sent)\n",
        "\n",
        "    # convert to lowercase and remove punctuations and symbols\n",
        "    # take if all characters in the string are alphabets and then decapitalize\n",
        "    sent = [w.lower() for w in words if w.isalpha()] \n",
        "\n",
        "    # remove stopwords\n",
        "    if rm_stopwords:\n",
        "      sent = [w for w in sent if w not in self.lst_stopwords]    \n",
        "\n",
        "    # apply stemming \n",
        "    if stemming:\n",
        "      sent = [self.ps.stem(w) for w in sent]\n",
        "\n",
        "    # apply lemmatization \n",
        "    if lemmatization:\n",
        "      sent = [self.wordnet_lemmatizer.lemmatize(w, pos = \"n\") for w in sent]\n",
        "      sent = [self.wordnet_lemmatizer.lemmatize(w, pos = \"v\") for w in sent]\n",
        "      sent = [self.wordnet_lemmatizer.lemmatize(w, pos = (\"a\")) for w in sent]\n",
        "\n",
        "    sent = \" \".join(sent)\n",
        "    return sent  \n",
        "\n",
        "\n",
        "  # add a new column to show how question parsing has done through normalization above\n",
        "  # for Tabular representation of the dataset\n",
        "  def norm_data(self, data):   \n",
        "    data.loc[:, \"question_processed\"] = data[\"question\"].apply(lambda x: self._norm_sent(x, rm_stopwords = True, lemmatization = True, stemming = True))\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "  # vectorization - fit vectorizer to training data\n",
        "  def bow_fit(self, corpus, type = \"tf\", max_features = 10000, ngram_range = (1,2)):\n",
        "\n",
        "    if type == \"tf\":\n",
        "      self.count_vectorizer = feature_extraction.text.CountVectorizer(max_features = max_features, ngram_range = ngram_range)\n",
        "      self.count_vectorizer.fit(corpus[\"question_processed\"])\n",
        "\n",
        "      # create a reverse mapping for the vocab\n",
        "      self.inv_count_vectorizer_vocab = {}\n",
        "      for label, ind in self.count_vectorizer.vocabulary_.items():\n",
        "        self.inv_count_vectorizer_vocab[ind] = label\n",
        "\n",
        "    elif type == \"tfidf\":\n",
        "      self.tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(max_features = max_features, ngram_range = ngram_range)\n",
        "      self.tfidf_vectorizer.fit(corpus[\"question_processed\"])\n",
        "      \n",
        "      # create a reverse mapping for the vocab\n",
        "      self.inv_tfidf_vectorizer_vocab = {}\n",
        "      for label, ind in self.tfidf_vectorizer.vocabulary_.items():\n",
        "        self.inv_tfidf_vectorizer_vocab[ind] = label\n",
        "\n",
        "    else:\n",
        "      return NotImplementedError\n",
        "\n",
        "\n",
        "  # transformation\n",
        "  def bow_transform(self, data, type = \"tf\"):\n",
        "    if type == \"tf\":\n",
        "      return self.count_vectorizer.transform(data[\"question_processed\"])\n",
        "    elif type == \"tfidf\":\n",
        "      return self.tfidf_vectorizer.transform(data[\"question_processed\"])\n",
        "    else:\n",
        "      return NotImplementedError\n",
        "\n",
        "\n",
        "  # category maps\n",
        "  def category_to_int(self, data):\n",
        "    return data.category.map(lambda x: self.category_map[x])\n",
        "\n",
        "  # literal maps\n",
        "  def literal_to_int(self, data):\n",
        "    return data.type.map(lambda x: self.literal_map[x[0]])\n",
        "\n",
        "\n",
        "  # distribute type by ontology class and encode missing if none\n",
        "  def type_to_int(self, data, type_no):\n",
        "    return data.type.map(\n",
        "        lambda x: self.type_maps[f\"type{type_no}\"][x[type_no - 1]] \n",
        "        if len(x) >= type_no \n",
        "        else self.type_maps[f\"type{type_no}\"][\"missing\"]\n",
        "        )\n",
        "\n",
        "\n",
        "  # resource maps\n",
        "  def add_type_maps(self, train_data, depth = 6, save = True, path = \"resource_types/\"):\n",
        "\n",
        "    levels = range(1, depth)\n",
        "    \n",
        "    if save:\n",
        "      os.makedirs(path, exist_ok = True)\n",
        "  \n",
        "    for l in levels:\n",
        "      type_name = f\"type{l}\"\n",
        "      self.type_maps[type_name] = {}\n",
        "      self.invtype_maps[type_name] = {}\n",
        "      ind = 0\n",
        "      temp_df = train_data[train_data[\"category\"] == \"resource\"][\"type\"].map(lambda x: x[l-1] if len(x) >= l else \"missing\").to_frame(type_name)\n",
        "      for ontology in temp_df[type_name]:\n",
        "        if (ontology not in self.type_maps[type_name]) and (ontology != \"missing\"):\n",
        "          self.type_maps[type_name][ontology] = ind \n",
        "          self.invtype_maps[type_name][ind] = ontology\n",
        "          ind += 1\n",
        "      if save:\n",
        "        with open(os.path.join(\"resource_types\", f\"type{l}_map.json\"), \"w\") as outfile:\n",
        "          temp_json_obj = json.dump(self.type_maps, outfile)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # save output\n",
        "  def save_vectorizers(self, path):\n",
        "\n",
        "    # make sure directory exists\n",
        "    os.makedirs(exist_ok= True, name=path)\n",
        "\n",
        "    if self.count_vectorizer != None:\n",
        "      with open(os.path.join(path, \"count_vectorizer.pkl\"), \"wb\") as count_file:\n",
        "        pickle.dump(self.count_vectorizer, count_file)\n",
        "    if self.tfidf_vectorizer != None:\n",
        "      with open(os.path.join(path, \"tfidf_vectorizer.pkl\"), \"wb\") as tfidf_file:\n",
        "        pickle.dump(self.tfidf_vectorizer, tfidf_file)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRRJ_AlIwcCS"
      },
      "source": [
        "etl = ETL()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKbfRa2a80BP"
      },
      "source": [
        "# split dataset\n",
        "# for validation 8:2\n",
        "# df_train, df_val = etl.split_data(merged_data)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVWgYwQS80ZZ"
      },
      "source": [
        "# text normalization\n",
        "# df_train = etl.norm_data(df_train)   # training set without val\n",
        "df_train = etl.norm_data(merged_data) # training set including val (total)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YJAsGNPC9Dv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "e0ffd244-0b94-4aa6-d12f-5216ac884194"
      },
      "source": [
        "df_train[['question', 'question_processed']]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>question_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was Jacqueline Kennedy Onassis a follower of M...</td>\n",
              "      <td>jacquelin kennedi onassi follow melkit greek c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the name of the opera based on Twelfth...</td>\n",
              "      <td>what name opera base twelfth night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When did Lena Horne receive the Grammy Award f...</td>\n",
              "      <td>when lena horn receiv grammi award best jazz v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Do Prince Harry and Prince William have the sa...</td>\n",
              "      <td>princ harri princ william parent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the subsidiary company working for Leo...</td>\n",
              "      <td>what subsidiari compani work leonard maltin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36665</th>\n",
              "      <td>what kinds of music is played by season's end</td>\n",
              "      <td>what kind music play season end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36666</th>\n",
              "      <td>which asteroid group is 6753 fursenko a member...</td>\n",
              "      <td>which asteroid group fursenko member</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36667</th>\n",
              "      <td>What language is azhakiya ravanan filmed in?</td>\n",
              "      <td>what languag azhakiya ravanan film</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36668</th>\n",
              "      <td>which position did herby fortunat play in foot...</td>\n",
              "      <td>which posit herbi fortunat play footbal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36669</th>\n",
              "      <td>who is a person that was born in sao paulo</td>\n",
              "      <td>who person bear sao paulo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39556 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                question                                 question_processed\n",
              "0      Was Jacqueline Kennedy Onassis a follower of M...  jacquelin kennedi onassi follow melkit greek c...\n",
              "1      What is the name of the opera based on Twelfth...                 what name opera base twelfth night\n",
              "2      When did Lena Horne receive the Grammy Award f...  when lena horn receiv grammi award best jazz v...\n",
              "3      Do Prince Harry and Prince William have the sa...                   princ harri princ william parent\n",
              "4      What is the subsidiary company working for Leo...        what subsidiari compani work leonard maltin\n",
              "...                                                  ...                                                ...\n",
              "36665      what kinds of music is played by season's end                    what kind music play season end\n",
              "36666  which asteroid group is 6753 fursenko a member...               which asteroid group fursenko member\n",
              "36667       What language is azhakiya ravanan filmed in?                 what languag azhakiya ravanan film\n",
              "36668  which position did herby fortunat play in foot...            which posit herbi fortunat play footbal\n",
              "36669         who is a person that was born in sao paulo                          who person bear sao paulo\n",
              "\n",
              "[39556 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku3cAUNXuzUG"
      },
      "source": [
        "# vectorization - bag of words model\n",
        "etl.bow_fit(corpus = df_train, type = \"tfidf\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3kClX11JQsW"
      },
      "source": [
        "etl.save_vectorizers(path=\"sklearn_objects\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpGAtqqku-Mb"
      },
      "source": [
        "## 3. Category Prediction Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNrKgk6Cu9b8"
      },
      "source": [
        "# set category prediction dataset\n",
        "X_train_category = etl.bow_transform(df_train, type = \"tfidf\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_K09HhAb4Vy"
      },
      "source": [
        "y_train_category = etl.category_to_int(df_train)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjARPmJfv84w"
      },
      "source": [
        "***3 different model in total: 1 for category, 1 for literal, 1 for resource***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-2QymUbxPNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d27e6cc-b4e7-43e2-e66e-ebc11924eaf5"
      },
      "source": [
        "# model for category classification\n",
        "clf_category = LogisticRegression(\n",
        "    random_state=seed, penalty = 'elasticnet', solver = 'saga',\n",
        "    l1_ratio = 0.2, n_jobs = -1, verbose = 2)\\\n",
        "    .fit(X_train_category, y_train_category)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 20 epochs took 72 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.2min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhcXhB1491gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad8c358-b348-4173-beca-f53fbd65652f"
      },
      "source": [
        "clf_category.score(X_train_category, y_train_category)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9665031853574679"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6t3q77QvuOE"
      },
      "source": [
        "## 4-1. Type Prediction Task - Literal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi_yxC3C5nrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52687a89-a828-4422-c766-fb9018e4c3be"
      },
      "source": [
        "# model for literal classification\n",
        "# get which rows are for literal only  \n",
        "train_literal_rows = (df_train[\"category\"] == \"literal\").values\n",
        "y_train_literal = etl.literal_to_int(df_train[train_literal_rows])\n",
        "\n",
        "clf_literal = LogisticRegression(\n",
        "    random_state=seed, penalty = 'elasticnet', solver = 'saga',\n",
        "    l1_ratio = 0.5, n_jobs = -1, verbose = 2\n",
        "    )\\\n",
        "    .fit(X_train_category[train_literal_rows, :], y_train_literal)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convergence after 20 epochs took 1 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihsyVY143UPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b0e6f6-d3c1-407a-f48a-385d1552e002"
      },
      "source": [
        "clf_literal.score(X_train_category[train_literal_rows], y_train_literal)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9508356004663816"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr-4DSgGGYoF"
      },
      "source": [
        "## 4-2. Type Prediction Task - Resource\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bLD1-vEVYWj"
      },
      "source": [
        "***Identify types of resources in the train data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZe_y3EeIYvz"
      },
      "source": [
        "etl.add_type_maps(df_train)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PorG_IvOK4_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e798097-0059-485d-b48c-5341513a8cdb"
      },
      "source": [
        "resource_models = []\n",
        "\n",
        "for l in range(1, 6):\n",
        "  # model for resource classification\n",
        "  # get which rows are for resource only\n",
        "  # must only include rows that have the type at the level \n",
        "  # of classification \n",
        "  train_resource_rows = ((df_train[\"category\"] == \"resource\") & (df_train[\"type\"].map(lambda x: len(x)) >= l)).values\n",
        "  y_train_type = etl.type_to_int(df_train[train_resource_rows], type_no=l)\n",
        "\n",
        "  clf_type = MLPClassifier(\n",
        "      random_state=seed, max_iter=10, hidden_layer_sizes=(1000, 500, 300)\n",
        "      , verbose = 2).\\\n",
        "    fit(X_train_category[train_resource_rows], y_train_type)\n",
        "\n",
        "  resource_models.append(clf_type)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.60073361\n",
            "Iteration 2, loss = 1.28514271\n",
            "Iteration 3, loss = 0.84813352\n",
            "Iteration 4, loss = 0.56595047\n",
            "Iteration 5, loss = 0.39063045\n",
            "Iteration 6, loss = 0.27895499\n",
            "Iteration 7, loss = 0.21972769\n",
            "Iteration 8, loss = 0.17741711\n",
            "Iteration 9, loss = 0.15124181\n",
            "Iteration 10, loss = 0.13678925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.57460801\n",
            "Iteration 2, loss = 1.24568463\n",
            "Iteration 3, loss = 0.78632519\n",
            "Iteration 4, loss = 0.53269140\n",
            "Iteration 5, loss = 0.38478882\n",
            "Iteration 6, loss = 0.29064727\n",
            "Iteration 7, loss = 0.23438973\n",
            "Iteration 8, loss = 0.19733377\n",
            "Iteration 9, loss = 0.17495153\n",
            "Iteration 10, loss = 0.16067453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.55699905\n",
            "Iteration 2, loss = 1.38819738\n",
            "Iteration 3, loss = 0.96441829\n",
            "Iteration 4, loss = 0.68901965\n",
            "Iteration 5, loss = 0.51262370\n",
            "Iteration 6, loss = 0.40982330\n",
            "Iteration 7, loss = 0.34204085\n",
            "Iteration 8, loss = 0.29101329\n",
            "Iteration 9, loss = 0.25413398\n",
            "Iteration 10, loss = 0.23677822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.80221215\n",
            "Iteration 2, loss = 1.66363582\n",
            "Iteration 3, loss = 1.19419448\n",
            "Iteration 4, loss = 0.86773169\n",
            "Iteration 5, loss = 0.65286595\n",
            "Iteration 6, loss = 0.52186810\n",
            "Iteration 7, loss = 0.42829741\n",
            "Iteration 8, loss = 0.37436630\n",
            "Iteration 9, loss = 0.32832508\n",
            "Iteration 10, loss = 0.29308867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.37953779\n",
            "Iteration 2, loss = 1.49619754\n",
            "Iteration 3, loss = 1.12289247\n",
            "Iteration 4, loss = 0.83189347\n",
            "Iteration 5, loss = 0.64235405\n",
            "Iteration 6, loss = 0.50297702\n",
            "Iteration 7, loss = 0.41489779\n",
            "Iteration 8, loss = 0.34872976\n",
            "Iteration 9, loss = 0.31019767\n",
            "Iteration 10, loss = 0.28594132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fxgGgeOMM9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d483230-046e-4049-c153-19bfd2683514"
      },
      "source": [
        "clf_type.score(X_train_category[train_resource_rows], y_train_type) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9329049123189195"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwcZ38iu_YYH"
      },
      "source": [
        "## 4. Save models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJnfBXgkfyUr"
      },
      "source": [
        "with open(os.path.join(\"sklearn_objects\", \"category_model.pkl\"), \"wb\") as mdl_file:\n",
        "  pickle.dump(clf_category, mdl_file)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_QaOf94RtQs"
      },
      "source": [
        "with open(os.path.join(\"sklearn_objects\", \"literal_model.pkl\"), \"wb\") as mdl_file:\n",
        "  pickle.dump(clf_literal, mdl_file)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASFCv0nkaphR"
      },
      "source": [
        "for l in range(1,6):\n",
        "  with open(os.path.join(\"sklearn_objects\", f\"resource_level_{l}_model.pkl\"), \"wb\") as mdl_file:\n",
        "    pickle.dump(resource_models[l-1], mdl_file)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pol2B42Cw2ck"
      },
      "source": [
        "## 5. Results & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV76HT3oUoxa"
      },
      "source": [
        "***Load Pre-trained models***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmutptNCTqbc"
      },
      "source": [
        "with open(\"sklearn_objects/category_model.pkl\", \"rb\") as clf_cat_file:\n",
        "  clf_category = pickle.load(clf_cat_file)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxMJq2xsBOwL"
      },
      "source": [
        "with open(\"sklearn_objects/literal_model.pkl\", \"rb\") as clf_lit_file:\n",
        "  clf_literal = pickle.load(clf_lit_file)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jx_Y4PaBQ0q"
      },
      "source": [
        "resource_models = []\n",
        "for l in range(1,6):\n",
        "  with open(f\"sklearn_objects/resource_level_{l}_model.pkl\", \"rb\") as res_mdl:\n",
        "    resource_models.append(pickle.load(res_mdl))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP9mCDz0BTeD"
      },
      "source": [
        "***Load processing class***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYLTHE0gUll3"
      },
      "source": [
        "etl = ETL(path_to_type_maps=\"resource_types\", path_to_vectorizers=\"sklearn_objects\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orIFdQaDN9ba"
      },
      "source": [
        "class ModelEvaluation:\n",
        "  def __init__(self, etl_inst, cat_model, lit_model, res_models):\n",
        "    self.etl_inst = etl_inst\n",
        "    self.cat_model = cat_model\n",
        "    self.lit_model = lit_model\n",
        "    self.res_models = res_models\n",
        "  \n",
        "  # X is a df\n",
        "  def get_predictions(self, X, bow_type = \"tf\"):\n",
        "\n",
        "    X = X.copy()\n",
        "\n",
        "    X.reset_index(inplace = True, drop = True)\n",
        "    \n",
        "    X_norm = self.etl_inst.norm_data(X)\n",
        "    X_vec = self.etl_inst.bow_transform(X_norm, type = bow_type)\n",
        "\n",
        "    bool_int = self.etl_inst.category_map[\"boolean\"]\n",
        "    literal_int = self.etl_inst.category_map[\"literal\"]\n",
        "    resource_int = self.etl_inst.category_map[\"resource\"]\n",
        "\n",
        "    cat_pred = self.cat_model.predict(X_vec)\n",
        "\n",
        "    ind_bool = cat_pred == bool_int\n",
        "    ind_literal = cat_pred == literal_int\n",
        "    ind_resource = cat_pred == resource_int\n",
        "\n",
        "    if len(ind_bool) > 0:\n",
        "      X.loc[ind_bool, \"cat_prediction\"] = \"boolean\"\n",
        "      X.loc[ind_bool, \"type_prediction\"] = pd.Series(\n",
        "          cat_pred[ind_bool], name = \"type_prediction\")\\\n",
        "          .map(lambda x: [\"boolean\"]).values\n",
        "\n",
        "    if len(ind_literal) > 0:\n",
        "      X.loc[ind_literal, \"cat_prediction\"] = \"literal\"\n",
        "      literal_pred = self.lit_model.predict(X_vec[ind_literal])\n",
        "      X.loc[ind_literal, \"type_prediction\"] = pd.Series(\n",
        "          literal_pred, name = \"type_prediction\")\\\n",
        "          .map(lambda x: [self.etl_inst.inv_literal_map[x]]).values\n",
        "\n",
        "    if len(ind_resource) > 0:\n",
        "      resource_preds = []\n",
        "      for ind, type_model in enumerate(self.res_models):\n",
        "          resource_preds.append(\n",
        "            pd.Series(\n",
        "                type_model.predict(X_vec[ind_resource]), name = f\"type_{ind}\").\\\n",
        "                map(lambda x: self.etl_inst.invtype_maps[f\"type{ind+1}\"][x])\n",
        "                )\n",
        "      resource_preds = pd.Series(pd.concat(resource_preds, axis = 1).values.tolist(), name = \"type_prediction\")\n",
        "      X.loc[ind_resource, \"type_prediction\"] = resource_preds.values\n",
        "      X.loc[ind_resource, \"cat_prediction\"] = \"resource\"\n",
        "      \n",
        "      return X\n",
        "\n",
        "  def output_predictions(self):\n",
        "    return NotImplementedError"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6BLutJ8jA-M"
      },
      "source": [
        "me = ModelEvaluation(etl, clf_category, clf_literal, resource_models)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b_rDe2EKOry"
      },
      "source": [
        "***Validate***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00lDYJIE0pit"
      },
      "source": [
        "# out_val = me.get_predictions(df_val, bow_type = 'tfidf')\n",
        "\n",
        "# true_output = out_val.loc[:, [\"id\", \"question\", \"category\", \"type\"]]\n",
        "# true_output_dict = [pred for ind, pred in true_output.to_dict(orient = \"index\").items()]\n",
        "\n",
        "# system_output = out_val.loc[:, [\"id\", \"cat_prediction\", \"type_prediction\"]]\n",
        "# system_output.columns = [\"id\", \"category\", \"type\"]\n",
        "# system_output_dict = [pred for ind, pred in system_output.to_dict(orient = \"index\").items()]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9raG_KIyB1x"
      },
      "source": [
        "***Run Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtiKCQsPxH_V"
      },
      "source": [
        "# os.makedirs(\"system_output/\", exist_ok = True)\n",
        "# with open(os.path.join(\"system_output\", \"ground_truth_json.json\"), \"w\") as gfile:\n",
        "#   json.dump(true_output_dict, gfile)\n",
        "\n",
        "# with open(os.path.join(\"system_output\", \"system_output_json.json\"), \"w\") as sfile:\n",
        "#   json.dump(system_output_dict, sfile)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPusetmQx_T7"
      },
      "source": [
        "# !python evaluate.py --type_hierarchy_tsv dbpedia_types.tsv  \\\n",
        "#  --ground_truth_json system_output/ground_truth_json.json \\\n",
        "#  --system_output_json system_output/system_output_json.json"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plQTw8kzKQmC"
      },
      "source": [
        "***Save Test Output***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvHRpUYDKHm6"
      },
      "source": [
        "out_test = me.get_predictions(test_data2021, bow_type = 'tfidf')\n",
        "\n",
        "#true_output = out_test.loc[:, [\"id\", \"question\"]]\n",
        "#true_output_dict = [pred for ind, pred in true_output.to_dict(orient = \"index\").items()]\n",
        "\n",
        "system_output = out_test.loc[:, [\"id\", \"cat_prediction\", \"type_prediction\"]]\n",
        "system_output.columns = [\"id\", \"category\", \"type\"]\n",
        "system_output_dict = [pred for ind, pred in system_output.to_dict(orient = \"index\").items()]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ4WgtQq2k1g"
      },
      "source": [
        "os.makedirs(\"system_output/\", exist_ok = True)\n",
        "# with open(os.path.join(\"system_output\", \"ground_truth_json.json\"), \"w\") as gfile:\n",
        "#   json.dump(true_output_dict, gfile)\n",
        "\n",
        "with open(os.path.join(\"system_output\", \"system_output_json.json\"), \"w\") as sfile:\n",
        "  json.dump(system_output_dict, sfile)"
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}