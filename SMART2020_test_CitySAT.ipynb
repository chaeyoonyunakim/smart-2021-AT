{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMART2020_test_CitySAT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOYvAoFAjuYxFICcgLRdms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaeyoonyunakim/smart-2021-AT/blob/main/SMART2020_test_CitySAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1YCTaqmYYfV"
      },
      "source": [
        "## 0. Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMErWmmfkxqp",
        "outputId": "29a55e60-b363-460d-d3fd-d886c3b8d33a"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "\n",
        "# authorization\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# locate dataset folder\n",
        "%ls -l '/content/drive/My Drive/2021_INM363_SMART/task1_dbpedia_train.json'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "-rw------- 1 root root 9263124 Sep 29 08:07 '/content/drive/My Drive/2021_INM363_SMART/task1_dbpedia_train.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqGDQCAWfPtF"
      },
      "source": [
        "# set working directory\n",
        "import os\n",
        "os.chdir(path = \"/content/drive/My Drive/2021_INM363_SMART/\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1KBaWJJk4Qg"
      },
      "source": [
        "# import basics\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "seed = 20211001\n",
        "import regex as re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qghbo96ZlCTN",
        "outputId": "4f45910c-6e90-47f6-f4a3-1e2944ad8a28"
      },
      "source": [
        "# import nlp relevants\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "set(stopwords.words('english'))\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# for bag-of-words (bow)\n",
        "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGJtj4fPlFid"
      },
      "source": [
        "# import scikit-learn tools for modelling and evaluation\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApUkA-SzlD3c"
      },
      "source": [
        "# import algos\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUs8fZVzZnq2"
      },
      "source": [
        "## 1. Data Loading & Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRL8r9it-bla"
      },
      "source": [
        "***Filter ''o'' in question out***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1W4YPu2wBEo"
      },
      "source": [
        "ap_re = re.compile(\"(^\\')(.*)(\\'$)\")\n",
        "\n",
        "def remove_appostrophe(x):\n",
        "  matches = ap_re.findall(x)\n",
        "  if len(matches) == 1:\n",
        "    return matches[0][1]\n",
        "  else:\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7bJ0m9W-XH7"
      },
      "source": [
        "***Load Dataset*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWVZEsmFwBLW"
      },
      "source": [
        "def load_data(path, train = True):\n",
        "  data = pd.read_json(path)\n",
        "  \n",
        "  # map na strings to nan\n",
        "  data.loc[:, \"question\"].replace(\"n/a\", np.nan, inplace = True)\n",
        "  \n",
        "  # drop na in data\n",
        "  if train:\n",
        "    data.dropna(subset=['id', 'question', 'category'], inplace=True)\n",
        "  else:\n",
        "    data.dropna(subset=['id', 'question'], inplace=True)\n",
        "\n",
        "  # remove apostrophes from the start and end of str\n",
        "  data.loc[:, \"question\"] = data[\"question\"].map(lambda x: remove_appostrophe(x))\n",
        "\n",
        "  # for the training data remove rows that have no types\n",
        "  if train:\n",
        "    data = data[data[\"type\"].map(lambda x : len(x) != 0)]\n",
        "\n",
        "  return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw9SNMabwBRI"
      },
      "source": [
        "# load dataset\n",
        "train_data2020 = load_data('/content/drive/My Drive/2021_INM363_SMART/smarttask_dbpedia_train.json')\n",
        "test_data2020 = load_data('/content/drive/My Drive/2021_INM363_SMART/smarttask_dbpedia_test.json', train = False)\n",
        "\n",
        "# train_data2021 = load_data('/content/drive/My Drive/2021_INM363_SMART/task1_dbpedia_train.json')\n",
        "# test_data2021 = load_data('/content/drive/My Drive/2021_INM363_SMART/task1_dbpedia_test.json', train = False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t3x_V5ogknH",
        "outputId": "a2b7c8a4-f177-4f01-86c8-1d78ddf93d90"
      },
      "source": [
        "# check data size\n",
        "train_data2020.shape, test_data2020.shape#, train_data2021.shape, test_data2021.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17482, 4), (4378, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data2020.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "V7GdYvSfaOcM",
        "outputId": "438e6bc8-33a0-4898-cff5-5af329aad1f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dbpedia_1177</td>\n",
              "      <td>Was Jacqueline Kennedy Onassis a follower of M...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dbpedia_14427</td>\n",
              "      <td>What is the name of the opera based on Twelfth...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:Opera, dbo:MusicalWork, dbo:Work]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dbpedia_16615</td>\n",
              "      <td>When did Lena Horne receive the Grammy Award f...</td>\n",
              "      <td>literal</td>\n",
              "      <td>[date]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dbpedia_23480</td>\n",
              "      <td>Do Prince Harry and Prince William have the sa...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dbpedia_3681</td>\n",
              "      <td>What is the subsidiary company working for Leo...</td>\n",
              "      <td>resource</td>\n",
              "      <td>[dbo:EducationalInstitution, dbo:Organisation,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  ...                                               type\n",
              "0   dbpedia_1177  ...                                          [boolean]\n",
              "1  dbpedia_14427  ...             [dbo:Opera, dbo:MusicalWork, dbo:Work]\n",
              "2  dbpedia_16615  ...                                             [date]\n",
              "3  dbpedia_23480  ...                                          [boolean]\n",
              "4   dbpedia_3681  ...  [dbo:EducationalInstitution, dbo:Organisation,...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data2020.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RYEkpMZtaTm_",
        "outputId": "9d3fc1b4-73b2-4146-d6af-80122a7a2ec6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>category</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dbpedia_16015</td>\n",
              "      <td>How many ingredients are in the grain} ?</td>\n",
              "      <td>literal</td>\n",
              "      <td>[number]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dbpedia_3885</td>\n",
              "      <td>Is the case fatality rate of Fournier gangrene...</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dbpedia_12907</td>\n",
              "      <td>Does the shelf life of spinach equal 8?</td>\n",
              "      <td>boolean</td>\n",
              "      <td>[boolean]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dbpedia_7955</td>\n",
              "      <td>What sound does a pig make in the French langu...</td>\n",
              "      <td>literal</td>\n",
              "      <td>[string]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dbpedia_2376</td>\n",
              "      <td>When was Fergie completed his record label in ...</td>\n",
              "      <td>literal</td>\n",
              "      <td>[date]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  ...       type\n",
              "0  dbpedia_16015  ...   [number]\n",
              "1   dbpedia_3885  ...  [boolean]\n",
              "2  dbpedia_12907  ...  [boolean]\n",
              "3   dbpedia_7955  ...   [string]\n",
              "4   dbpedia_2376  ...     [date]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3FaIg_tHHC"
      },
      "source": [
        "## 2. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7LfNufdhaCV"
      },
      "source": [
        "***Extract, Transform, Load (ETL)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epe56ypal-qk"
      },
      "source": [
        "class ETL:\n",
        "\n",
        "###############################################################################\n",
        "#   Global Variables     \n",
        "###############################################################################\n",
        "\n",
        "  # text normalization - stemming, lemmatization, stopwords\n",
        "  ps = PorterStemmer()\n",
        "  wordnet_lemmatizer = WordNetLemmatizer() \n",
        "  s_words = stopwords.words()\n",
        "\n",
        "  # set default feature_extraction parameters\n",
        "  count_vectorizer = None \n",
        "  inv_count_vectorizer_vocab = None\n",
        "  tfidf_vectorizer = None\n",
        "  inv_tfidf_vectorizer_vocab = None\n",
        "\n",
        "  # category maps\n",
        "  category_map = {\"boolean\": 0, \"resource\": 1, \"literal\": 2}\n",
        "  inv_category_map = {}\n",
        "\n",
        "  for label, ind in category_map.items():\n",
        "    inv_category_map[ind] = label\n",
        "\n",
        "  # literal maps\n",
        "  literal_map = {\"date\": 0, \"string\": 1, \"number\": 2}\n",
        "  inv_literal_map = {}\n",
        "\n",
        "  for label, ind in literal_map.items():\n",
        "    inv_literal_map[ind] = label  \n",
        "\n",
        "  # resource maps\n",
        "  type_maps = {}\n",
        "  invtype_maps = {}\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#   Main\n",
        "###############################################################################\n",
        "\n",
        "  def __init__(self, path_to_type_maps = None, path_to_vectorizers = None):\n",
        "\n",
        "    # load type maps if requested\n",
        "    if path_to_type_maps != None:\n",
        "      base_dir = path_to_type_maps\n",
        "      paths = [fp for fp in os.listdir(base_dir) if \"type\" in fp]\n",
        "\n",
        "      for fp in paths:\n",
        "        with open(os.path.join(base_dir, fp), \"r\") as input_file:\n",
        "          type_name = fp.split(\"_\")[0]\n",
        "          self.invtype_maps[type_name] = {}\n",
        "          self.type_maps[type_name] = json.load(input_file)[type_name]\n",
        "          for ontology, ind in self.type_maps[type_name].items():\n",
        "            self.invtype_maps[type_name][ind] = ontology\n",
        "\n",
        "    # load data vectorizers if requested\n",
        "    if path_to_vectorizers != None:\n",
        "      base_dir = path_to_vectorizers\n",
        "      paths = [fp for fp in os.listdir(base_dir) if \"vectorizer\" in fp]\n",
        "\n",
        "      for fp in paths:\n",
        "        vectorizer_name = fp.split(\"_\")[0]\n",
        "        with open(os.path.join(base_dir, fp), \"rb\") as input_file:\n",
        "          if vectorizer_name == \"count\":\n",
        "            self.count_vectorizer = pickle.load(input_file)\n",
        "          elif vectorizer_name == \"tfidf\":\n",
        "            self.tfidf_vectorizer = pickle.load(input_file)\n",
        "          else:\n",
        "            NotImplementedError\n",
        "\n",
        "\n",
        "  # split training dataset to exclude validation dataset\n",
        "  # set train:val = 8:2\n",
        "  def split_data(self, data, val_size = 0.2):\n",
        "    df_train, df_test = model_selection.train_test_split(data, test_size = val_size, random_state = seed)\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "  # normalization of question sentences\n",
        "  def _norm_sent(self, sent, rm_stopwords = False, stemming = True, lemmatization = False):\n",
        "    # tokenize - sentence to word\n",
        "    words = word_tokenize(sent)\n",
        "    # take if all characters in the string are alphabets and then decapitalize\n",
        "    sent = [w.lower() for w in words if w.isalpha()] \n",
        "\n",
        "    # remove stopwords\n",
        "    if rm_stopwords:\n",
        "      sent = [w for w in sent if w not in self.s_words]    \n",
        "\n",
        "    # apply lemmatization \n",
        "    if lemmatization:\n",
        "      sent = [self.wordnet_lemmatizer.lemmatize(w, pos = \"n\") for w in sent]\n",
        "      sent = [self.wordnet_lemmatizer.lemmatize(w, pos = \"v\") for w in sent]\n",
        "      sent = [self.wordnet_lemmatizer.lemmatize(w, pos = (\"a\")) for w in sent]\n",
        "\n",
        "    # apply stemming \n",
        "    if stemming:\n",
        "      sent = [self.ps.stem(w) for w in sent]\n",
        "\n",
        "    sent = \" \".join(sent)\n",
        "    return sent  \n",
        "\n",
        "\n",
        "  # add a new column to show how question parsing has done through normalization above\n",
        "  # for Tabular representation of the dataset\n",
        "  def norm_data(self, data):   \n",
        "    data.loc[:, \"question_processed\"] = data[\"question\"].apply(lambda x: self._norm_sent(x, rm_stopwords = False, lemmatization = True, stemming = True))\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "  # vectorization - fit vectorizer to training data\n",
        "  def bow_fit(self, corpus, type = \"tf\", max_features = 10000, ngram_range = (1,2)):\n",
        "\n",
        "    if type == \"tf\":\n",
        "      self.count_vectorizer = feature_extraction.text.CountVectorizer(max_features = max_features, ngram_range = ngram_range)\n",
        "      self.count_vectorizer.fit(corpus[\"question_processed\"])\n",
        "\n",
        "      # create a reverse mapping for the vocab\n",
        "      self.inv_count_vectorizer_vocab = {}\n",
        "      for label, ind in self.count_vectorizer.vocabulary_.items():\n",
        "        self.inv_count_vectorizer_vocab[ind] = label\n",
        "\n",
        "    elif type == \"tfidf\": \n",
        "      self.tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(max_features = max_features, ngram_range = ngram_range)\n",
        "      self.tfidf_vectorizer.fit(corpus[\"question_processed\"])\n",
        "      \n",
        "      # create a reverse mapping for the vocab\n",
        "      self.inv_tfidf_vectorizer_vocab = {}\n",
        "      for label, ind in self.tfidf_vectorizer.vocabulary_.items():\n",
        "        self.inv_tfidf_vectorizer_vocab[ind] = label\n",
        "\n",
        "    else:\n",
        "      return NotImplementedError\n",
        "\n",
        "\n",
        "  # transformation\n",
        "  def bow_transform(self, data, type = \"tf\"):\n",
        "    if type == \"tf\":\n",
        "      return self.count_vectorizer.transform(data[\"question_processed\"])\n",
        "    elif type == \"tfidf\":\n",
        "      return self.tfidf_vectorizer.transform(data[\"question_processed\"])\n",
        "    else:\n",
        "      return NotImplementedError\n",
        "\n",
        "\n",
        "  # category maps\n",
        "  def category_to_int(self, data):\n",
        "    return data.category.map(lambda x: self.category_map[x])\n",
        "\n",
        "  # literal maps\n",
        "  def literal_to_int(self, data):\n",
        "    return data.type.map(lambda x: self.literal_map[x[0]])\n",
        "\n",
        "\n",
        "  # distribute type by ontology class and encode missing if none\n",
        "  def type_to_int(self, data, type_no):\n",
        "    return data.type.map(\n",
        "        lambda x: self.type_maps[f\"type{type_no}\"][x[type_no - 1]] \n",
        "        if len(x) >= type_no \n",
        "        else self.type_maps[f\"type{type_no}\"][\"missing\"]\n",
        "        )\n",
        "\n",
        "\n",
        "  # resource maps\n",
        "  def add_type_maps(self, train_data, depth = 10, save = True, path = \"resource_types/\"):\n",
        "\n",
        "    levels = range(1, depth)\n",
        "    \n",
        "    if save:\n",
        "      os.makedirs(path, exist_ok = True)\n",
        "  \n",
        "    for l in levels:\n",
        "      type_name = f\"type{l}\"\n",
        "      self.type_maps[type_name] = {}\n",
        "      self.invtype_maps[type_name] = {}\n",
        "      ind = 0\n",
        "      temp_df = train_data[train_data[\"category\"] == \"resource\"][\"type\"].map(lambda x: x[l-1] if len(x) >= l else \"missing\").to_frame(type_name)\n",
        "      for ontology in temp_df[type_name]:\n",
        "        if (ontology not in self.type_maps[type_name]) and (ontology != \"missing\"):\n",
        "          self.type_maps[type_name][ontology] = ind \n",
        "          self.invtype_maps[type_name][ind] = ontology\n",
        "          ind += 1\n",
        "      if save:\n",
        "        with open(os.path.join(\"resource_types\", f\"type{l}_map.json\"), \"w\") as outfile:\n",
        "          temp_json_obj = json.dump(self.type_maps, outfile)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # save output\n",
        "  def save_vectorizers(self, path):\n",
        "\n",
        "    # make sure directory exists\n",
        "    os.makedirs(exist_ok= True, name=path)\n",
        "\n",
        "    if self.count_vectorizer != None:\n",
        "      with open(os.path.join(path, \"count_vectorizer.pkl\"), \"wb\") as count_file:\n",
        "        pickle.dump(self.count_vectorizer, count_file)\n",
        "    if self.tfidf_vectorizer != None:\n",
        "      with open(os.path.join(path, \"tfidf_vectorizer.pkl\"), \"wb\") as tfidf_file:\n",
        "        pickle.dump(self.tfidf_vectorizer, tfidf_file)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRRJ_AlIwcCS"
      },
      "source": [
        "etl = ETL()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKbfRa2a80BP"
      },
      "source": [
        "# split dataset\n",
        "# for validation 8:2\n",
        "# df_train, df_val = etl.split_data(train_data2020)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVWgYwQS80ZZ"
      },
      "source": [
        "# text normalization\n",
        "# df_train = etl.norm_data(df_train)   # training set without val\n",
        "df_train = etl.norm_data(train_data2020) # training set including val (total)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YJAsGNPC9Dv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "f28f0eb0-6bfc-4138-bc79-f3005c7c8da1"
      },
      "source": [
        "df_train[['question', 'question_processed']]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>question_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was Jacqueline Kennedy Onassis a follower of M...</td>\n",
              "      <td>wa jacquelin kennedi onassi a follow of melkit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the name of the opera based on Twelfth...</td>\n",
              "      <td>what be the name of the opera base on twelfth ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When did Lena Horne receive the Grammy Award f...</td>\n",
              "      <td>when do lena horn receiv the grammi award for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Do Prince Harry and Prince William have the sa...</td>\n",
              "      <td>do princ harri and princ william have the same...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the subsidiary company working for Leo...</td>\n",
              "      <td>what be the subsidiari compani work for leonar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17566</th>\n",
              "      <td>Is the flexural strain at break of the acrylon...</td>\n",
              "      <td>be the flexur strain at break of the acrylonit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17567</th>\n",
              "      <td>Where did Hilary Putnam receive their Ph.D.?</td>\n",
              "      <td>where do hilari putnam receiv their</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17568</th>\n",
              "      <td>Who replaced Charles Evans Hughes as the Chief...</td>\n",
              "      <td>who replac charl evan hugh a the chief justic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17569</th>\n",
              "      <td>Name the river with source as Columbia Lake an...</td>\n",
              "      <td>name the river with sourc a columbia lake and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17570</th>\n",
              "      <td>When will Selma Lagerlöf start their membershi...</td>\n",
              "      <td>when will selma lagerlöf start their membershi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17482 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                question                                 question_processed\n",
              "0      Was Jacqueline Kennedy Onassis a follower of M...  wa jacquelin kennedi onassi a follow of melkit...\n",
              "1      What is the name of the opera based on Twelfth...  what be the name of the opera base on twelfth ...\n",
              "2      When did Lena Horne receive the Grammy Award f...  when do lena horn receiv the grammi award for ...\n",
              "3      Do Prince Harry and Prince William have the sa...  do princ harri and princ william have the same...\n",
              "4      What is the subsidiary company working for Leo...  what be the subsidiari compani work for leonar...\n",
              "...                                                  ...                                                ...\n",
              "17566  Is the flexural strain at break of the acrylon...  be the flexur strain at break of the acrylonit...\n",
              "17567       Where did Hilary Putnam receive their Ph.D.?                where do hilari putnam receiv their\n",
              "17568  Who replaced Charles Evans Hughes as the Chief...  who replac charl evan hugh a the chief justic ...\n",
              "17569  Name the river with source as Columbia Lake an...  name the river with sourc a columbia lake and ...\n",
              "17570  When will Selma Lagerlöf start their membershi...  when will selma lagerlöf start their membershi...\n",
              "\n",
              "[17482 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku3cAUNXuzUG"
      },
      "source": [
        "# vectorization - bag of words model\n",
        "etl.bow_fit(corpus = df_train, type = \"tf\")\n",
        "etl.save_vectorizers(path=\"sklearn_objects\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpGAtqqku-Mb"
      },
      "source": [
        "## 3. Category Prediction Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNrKgk6Cu9b8"
      },
      "source": [
        "# set category prediction dataset\n",
        "X_train_category = etl.bow_transform(df_train)\n",
        "y_train_category = etl.category_to_int(df_train)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-2QymUbxPNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9859467-367f-4456-82a2-a7d884dca553"
      },
      "source": [
        "# model for category classification\n",
        "clf_category = LogisticRegression(\n",
        "    random_state=seed, penalty = 'elasticnet', solver = 'saga',\n",
        "    l1_ratio = 0.2, n_jobs = -1, verbose = 2)\\\n",
        "    .fit(X_train_category, y_train_category)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_iter reached after 141 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhcXhB1491gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6de9baf-50a1-4716-8796-85a68deb4231"
      },
      "source": [
        "clf_category.score(X_train_category, y_train_category)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9672806315066926"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6t3q77QvuOE"
      },
      "source": [
        "## 4-1. Type Prediction Task - Literal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjARPmJfv84w"
      },
      "source": [
        "***3 different model in total: 1 for category, 1 for literal, 1 for resource***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi_yxC3C5nrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c21dc2-5a40-4657-beed-9517156a2bd0"
      },
      "source": [
        "# model for literal classification\n",
        "# get which rows are for literal only  \n",
        "train_literal_rows = (df_train[\"category\"] == \"literal\").values\n",
        "y_train_literal = etl.literal_to_int(df_train[train_literal_rows])\n",
        "\n",
        "clf_literal = LogisticRegression(\n",
        "    random_state=seed, penalty = 'elasticnet', solver = 'saga',\n",
        "    l1_ratio = 0.5, n_jobs = -1, verbose = 2)\\\n",
        "    .fit(X_train_category[train_literal_rows, :], y_train_literal)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_iter reached after 8 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    7.4s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihsyVY143UPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9c70a0-984a-48d9-8ad3-6773931ee423"
      },
      "source": [
        "clf_literal.score(X_train_category[train_literal_rows], y_train_literal)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9798293250581847"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr-4DSgGGYoF"
      },
      "source": [
        "## 4-2. Type Prediction Task - Resource\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bLD1-vEVYWj"
      },
      "source": [
        "***Identify types of resources in the train data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZe_y3EeIYvz"
      },
      "source": [
        "etl.add_type_maps(df_train)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZU7YplswisE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c729d738-012d-47a1-8efc-f2f9e48f49e4"
      },
      "source": [
        "etl.type_maps.keys()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['type1', 'type2', 'type3', 'type4', 'type5', 'type6', 'type7', 'type8', 'type9'])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPYDqPkqw6KD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a015552-b739-472c-c7c0-663b87ce88cb"
      },
      "source": [
        "etl.type_maps.values()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([{'dbo:Opera': 0, 'dbo:EducationalInstitution': 1, 'dbo:State': 2, 'dbo:Country': 3, 'dbo:AcademicSubject': 4, 'dbo:Name': 5, 'dbo:Organisation': 6, 'dbo:Person': 7, 'dbo:WrittenWork': 8, 'dbo:EthnicGroup': 9, 'dbo:Museum': 10, 'dbo:Writer': 11, 'dbo:AmericanFootballPlayer': 12, 'dbo:ChemicalCompound': 13, 'dbo:OfficeHolder': 14, 'dbo:Magazine': 15, 'dbo:Award': 16, 'dbo:Animal': 17, 'dbo:Activity': 18, 'dbo:NobelPrize': 19, 'dbo:Single': 20, 'dbo:Work': 21, 'dbo:MetroStation': 22, 'dbo:Film': 23, 'dbo:Profession': 24, 'dbo:City': 25, 'dbo:Deity': 26, 'dbo:Galaxy': 27, 'dbo:Company': 28, 'dbo:Gene': 29, 'dbo:Contest': 30, 'dbo:TelevisionEpisode': 31, 'dbo:University': 32, 'dbo:MusicalArtist': 33, 'dbo:Scientist': 34, 'dbo:PoliticalParty': 35, 'dbo:MusicGenre': 36, 'dbo:Disease': 37, 'dbo:Taxon': 38, 'dbo:Book': 39, 'dbo:Settlement': 40, 'dbo:GovernmentAgency': 41, 'dbo:Ship': 42, 'dbo:Drug': 43, 'dbo:Island': 44, 'dbo:Mountain': 45, 'dbo:Village': 46, 'dbo:River': 47, 'dbo:Media': 48, 'dbo:Sport': 49, 'dbo:LegalCase': 50, 'dbo:HistoricalRegion': 51, 'dbo:MilitaryConflict': 52, 'dbo:SportsTeam': 53, 'dbo:TelevisionStation': 54, 'dbo:Archive': 55, 'dbo:Asteroid': 56, 'dbo:GivenName': 57, 'dbo:Software': 58, 'dbo:Monastery': 59, 'dbo:TelevisionShow': 60, 'dbo:MilitaryUnit': 61, 'dbo:Archipelago': 62, 'dbo:Saint': 63, 'dbo:RailwayLine': 64, 'dbo:Non-ProfitOrganisation': 65, 'dbo:Cemetery': 66, 'dbo:Tournament': 67, 'dbo:Aircraft': 68, 'dbo:SoccerPlayer': 69, 'dbo:Royalty': 70, 'dbo:Building': 71, 'dbo:Bridge': 72, 'dbo:Band': 73, 'dbo:Lake': 74, 'dbo:Family': 75, 'dbo:Spacecraft': 76, 'dbo:Church': 77, 'dbo:BoardGame': 78, 'dbo:Constellation': 79, 'dbo:Play': 80, 'dbo:Region': 81, 'dbo:VideoGame': 82, 'dbo:Castle': 83, 'dbo:BasketballPlayer': 84, 'dbo:InformationAppliance': 85, 'dbo:Election': 86, 'dbo:Protein': 87, 'dbo:Weapon': 88, 'dbo:ProgrammingLanguage': 89, 'dbo:Ideology': 90, 'dbo:BasketballTeam': 91, 'dbo:Album': 92, 'dbo:RailwayStation': 93, 'dbo:RecordLabel': 94, 'dbo:AdultActor': 95, 'dbo:Vein': 96, 'dbo:President': 97, 'dbo:SpaceMission': 98, 'dbo:Event': 99, 'dbo:SpaceStation': 100, 'dbo:Game': 101, 'dbo:Airport': 102, 'dbo:Parliament': 103, 'dbo:MedicalSpecialty': 104, 'dbo:Currency': 105, 'dbo:Planet': 106, 'dbo:FormulaOneRacer': 107, 'dbo:SportsClub': 108, 'dbo:RailwayTunnel': 109, 'dbo:WineRegion': 110, 'dbo:Newspaper': 111, 'dbo:Town': 112, 'dbo:HorseRace': 113, 'dbo:Manga': 114, 'dbo:Skyscraper': 115, 'dbo:BaseballTeam': 116, 'dbo:NobleFamily': 117, 'dbo:SportsLeague': 118, 'dbo:Novel': 119, 'dbo:Mammal': 120, 'dbo:FigureSkater': 121, 'dbo:NaturalRegion': 122, 'dbo:CyclingRace': 123, 'dbo:Hotel': 124, 'dbo:Road': 125, 'dbo:Stadium': 126, 'dbo:NationalAnthem': 127, 'dbo:Musical': 128, 'dbo:HockeyTeam': 129, 'dbo:Legislature': 130, 'dbo:MartialArtist': 131, 'dbo:Dam': 132, 'dbo:AcademicJournal': 133, 'dbo:MountainRange': 134, 'dbo:AnatomicalStructure': 135, 'dbo:Ocean': 136, 'dbo:SoccerClub': 137, 'dbo:FictionalCharacter': 138, 'dbo:Airline': 139, 'dbo:Language': 140, 'dbo:Locomotive': 141, 'dbo:Beverage': 142, 'dbo:MusicalWork': 143, 'dbo:Monarch': 144, 'dbo:PowerStation': 145, 'dbo:MountainPass': 146, 'dbo:AmericanFootballTeam': 147, 'dbo:Letter': 148, 'dbo:Mineral': 149, 'dbo:Cat': 150, 'dbo:CollegeCoach': 151, 'dbo:Boxer': 152, 'dbo:Architect': 153, 'dbo:SkiResort': 154, 'dbo:Philosopher': 155, 'dbo:Valley': 156, 'dbo:PopulatedPlace': 157, 'dbo:Senator': 158, 'dbo:Factory': 159, 'dbo:MilitaryPerson': 160, 'dbo:Gymnast': 161, 'dbo:Library': 162, 'dbo:Port': 163, 'dbo:Volcano': 164, 'dbo:Swimmer': 165, 'dbo:School': 166, 'dbo:Astronaut': 167, 'dbo:Food': 168, 'dbo:Venue': 169, 'dbo:Artist': 170, 'dbo:Politician': 171, 'dbo:Rocket': 172, 'dbo:Cave': 173, 'dbo:SportsManager': 174, 'dbo:GeologicalPeriod': 175, 'dbo:Judge': 176, 'dbo:AdministrativeRegion': 177, 'dbo:Square': 178, 'dbo:Monument': 179, 'dbo:Comedian': 180, 'dbo:SoccerManager': 181, 'dbo:Mollusca': 182, 'dbo:ComicsCreator': 183, 'dbo:Prison': 184, 'dbo:BeautyQueen': 185, 'dbo:Train': 186, 'dbo:Anime': 187, 'dbo:Enzyme': 188, 'dbo:Muscle': 189, 'dbo:Holiday': 190, 'dbo:RadioProgram': 191, 'dbo:BaseballLeague': 192, 'dbo:Ambassador': 193, 'dbo:Governor': 194, 'dbo:ComicsCharacter': 195, 'dbo:GolfCourse': 196, 'dbo:ProtectedArea': 197, 'dbo:PublicService': 198, 'dbo:Street': 199, 'dbo:LaunchPad': 200, 'dbo:RugbyPlayer': 201, 'dbo:Mosque': 202, 'dbo:Bird': 203, 'dbo:Broadcaster': 204, 'dbo:Plant': 205, 'dbo:BasketballLeague': 206, 'dbo:Tax': 207, 'dbo:Artery': 208, 'dbo:Bacteria': 209, 'dbo:Bank': 210, 'dbo:Restaurant': 211, 'dbo:PersonFunction': 212, 'dbo:TennisPlayer': 213, 'dbo:Bone': 214, 'dbo:ReligiousBuilding': 215, 'dbo:Publisher': 216, 'dbo:Motorcycle': 217, 'dbo:AutomobileEngine': 218, 'dbo:Poem': 219, 'dbo:Lighthouse': 220, 'dbo:IceHockeyLeague': 221, 'dbo:Tower': 222, 'dbo:HistoricBuilding': 223, 'dbo:Zoo': 224, 'dbo:Nerve': 225, 'dbo:HollywoodCartoon': 226, 'dbo:ArchitecturalStructure': 227, 'dbo:MusicFestival': 228, 'dbo:Brain': 229, 'dbo:Insect': 230, 'dbo:Song': 231, 'dbo:Agent': 232, 'dbo:ChessPlayer': 233, 'dbo:CultivatedVariety': 234, 'dbo:IceHockeyPlayer': 235, 'dbo:Hospital': 236, 'dbo:Flag': 237, 'dbo:FloweringPlant': 238, 'dbo:WorldHeritageSite': 239, 'dbo:HistoricalPeriod': 240, 'dbo:PublicTransitSystem': 241, 'dbo:DartsPlayer': 242, 'dbo:NaturalPlace': 243, 'dbo:Meeting': 244, 'dbo:BaseballPlayer': 245, 'dbo:AmericanFootballLeague': 246, 'dbo:TennisTournament': 247, 'dbo:Place': 248, 'dbo:Noble': 249, 'dbo:Sea': 250, 'dbo:Grape': 251, 'dbo:FilmFestival': 252, 'dbo:Continent': 253, 'dbo:PlayboyPlaymate': 254, 'dbo:BroadcastNetwork': 255, 'dbo:Priest': 256, 'dbo:HotSpring': 257, 'dbo:Actor': 258, 'dbo:MemberOfParliament': 259, 'dbo:Glacier': 260, 'dbo:Wrestler': 261, 'dbo:Beer': 262, 'dbo:CardGame': 263, 'dbo:Brewery': 264, 'dbo:RaceTrack': 265}, {'dbo:MusicalWork': 0, 'dbo:Organisation': 1, 'dbo:PopulatedPlace': 2, 'dbo:State': 3, 'dbo:TopicalConcept': 4, 'dbo:Agent': 5, 'dbo:Work': 6, 'dbo:Castle': 7, 'dbo:Person': 8, 'dbo:GridironFootballPlayer': 9, 'dbo:ChemicalSubstance': 10, 'dbo:President': 11, 'dbo:PeriodicalLiterature': 12, 'dbo:Eukaryote': 13, 'dbo:Award': 14, 'dbo:Station': 15, 'dbo:PersonFunction': 16, 'dbo:City': 17, 'dbo:Settlement': 18, 'dbo:CelestialBody': 19, 'dbo:Biomolecule': 20, 'dbo:Competition': 21, 'dbo:EducationalInstitution': 22, 'dbo:Artist': 23, 'dbo:Band': 24, 'dbo:Genre': 25, 'dbo:TelevisionShow': 26, 'dbo:WrittenWork': 27, 'dbo:MeanOfTransportation': 28, 'dbo:NaturalPlace': 29, 'dbo:Stream': 30, 'dbo:Game': 31, 'dbo:Case': 32, 'dbo:Region': 33, 'dbo:SocietalEvent': 34, 'dbo:BroadcastNetwork': 35, 'dbo:CollectionOfValuables': 36, 'dbo:Name': 37, 'dbo:ReligiousBuilding': 38, 'dbo:Cleric': 39, 'dbo:RouteOfTransportation': 40, 'dbo:Place': 41, 'dbo:SportsEvent': 42, 'dbo:Athlete': 43, 'dbo:OfficeHolder': 44, 'dbo:ArchitecturalStructure': 45, 'dbo:Country': 46, 'dbo:FormulaOneTeam': 47, 'dbo:Group': 48, 'dbo:Continent': 49, 'dbo:Film': 50, 'dbo:Sea': 51, 'dbo:Software': 52, 'dbo:Island': 53, 'dbo:Building': 54, 'dbo:Device': 55, 'dbo:Language': 56, 'dbo:SoccerClub': 57, 'dbo:Company': 58, 'dbo:Actor': 59, 'dbo:AnatomicalStructure': 60, 'dbo:Politician': 61, 'dbo:Activity': 62, 'dbo:Infrastructure': 63, 'dbo:RacingDriver': 64, 'dbo:PublicTransitSystem': 65, 'dbo:Comic': 66, 'dbo:SportsTeam': 67, 'dbo:Family': 68, 'dbo:Book': 69, 'dbo:Animal': 70, 'dbo:Race': 71, 'dbo:SportFacility': 72, 'dbo:Sport': 73, 'dbo:BodyOfWater': 74, 'dbo:BaseballTeam': 75, 'dbo:Deity': 76, 'dbo:AmericanFootballTeam': 77, 'dbo:MilitaryPerson': 78, 'dbo:Food': 79, 'dbo:Mammal': 80, 'dbo:Coach': 81, 'dbo:SkiArea': 82, 'dbo:Royalty': 83, 'dbo:SoccerPlayer': 84, 'dbo:TimePeriod': 85, 'dbo:Philosopher': 86, 'dbo:SportsManager': 87, 'dbo:SportsClub': 88, 'dbo:Cartoon': 89, 'dbo:SportsLeague': 90, 'dbo:Theatre': 91, 'dbo:FictionalCharacter': 92, 'dbo:Museum': 93, 'dbo:BasketballTeam': 94, 'dbo:SoccerLeague': 95, 'dbo:Species': 96, 'dbo:Engine': 97, 'dbo:Broadcaster': 98, 'dbo:Stadium': 99, 'dbo:Tower': 100, 'dbo:WinterSportPlayer': 101, 'dbo:Mountain': 102, 'dbo:TelevisionEpisode': 103, 'dbo:Writer': 104, 'dbo:Volcano': 105, 'dbo:PowerStation': 106, 'dbo:Insect': 107, 'dbo:ComicsCreator': 108, 'dbo:Plant': 109, 'dbo:Skyscraper': 110, 'dbo:School': 111, 'dbo:Airline': 112, 'dbo:Non-ProfitOrganisation': 113, 'dbo:Noble': 114, 'dbo:Church': 115, 'dbo:Monument': 116, 'dbo:Tournament': 117, 'dbo:Location': 118, 'dbo:River': 119, 'dbo:Town': 120, 'dbo:FloweringPlant': 121, 'dbo:RailwayStation': 122, 'dbo:MountainRange': 123, 'dbo:Beverage': 124}, {'dbo:Work': 0, 'dbo:Agent': 1, 'dbo:Place': 2, 'dbo:PopulatedPlace': 3, 'dbo:Building': 4, 'dbo:Athlete': 5, 'dbo:Politician': 6, 'dbo:WrittenWork': 7, 'dbo:Species': 8, 'dbo:Infrastructure': 9, 'dbo:Settlement': 10, 'dbo:Event': 11, 'dbo:Organisation': 12, 'dbo:Person': 13, 'dbo:TopicalConcept': 14, 'dbo:BodyOfWater': 15, 'dbo:Activity': 16, 'dbo:UnitOfWork': 17, 'dbo:State': 18, 'dbo:Broadcaster': 19, 'dbo:Location': 20, 'dbo:SocietalEvent': 21, 'dbo:SportsTeam': 22, 'dbo:AdministrativeRegion': 23, 'dbo:ArchitecturalStructure': 24, 'dbo:Artist': 25, 'dbo:OfficeHolder': 26, 'dbo:MotorsportRacer': 27, 'dbo:Plant': 28, 'dbo:SportsEvent': 29, 'dbo:Island': 30, 'dbo:ProtectedArea': 31, 'dbo:Group': 32, 'dbo:NaturalPlace': 33, 'dbo:Eukaryote': 34, 'dbo:River': 35, 'dbo:Animal': 36, 'dbo:SportFacility': 37, 'dbo:Cemetery': 38, 'dbo:Country': 39, 'dbo:MilitaryPerson': 40, 'dbo:President': 41, 'dbo:SoccerManager': 42, 'dbo:Venue': 43, 'dbo:ReligiousBuilding': 44, 'dbo:SportsLeague': 45, 'dbo:Company': 46, 'dbo:SoccerPlayer': 47, 'dbo:PublicTransitSystem': 48, 'dbo:Device': 49, 'dbo:MusicalArtist': 50, 'dbo:EducationalInstitution': 51, 'dbo:Band': 52, 'dbo:SoccerClub': 53, 'dbo:Station': 54, 'dbo:Volcano': 55, 'dbo:Food': 56, 'dbo:SportsManager': 57}, {'dbo:Location': 0, 'dbo:Place': 1, 'dbo:ArchitecturalStructure': 2, 'dbo:Person': 3, 'dbo:Work': 4, 'dbo:PopulatedPlace': 5, 'dbo:Agent': 6, 'dbo:NaturalPlace': 7, 'dbo:Organisation': 8, 'dbo:Event': 9, 'dbo:Athlete': 10, 'dbo:Eukaryote': 11, 'dbo:SocietalEvent': 12, 'dbo:CollegeCoach': 13, 'dbo:Species': 14, 'dbo:Building': 15, 'dbo:Bird': 16, 'dbo:Governor': 17, 'dbo:Settlement': 18, 'dbo:TelevisionShow': 19, 'dbo:Airport': 20, 'dbo:Band': 21, 'dbo:Insect': 22, 'dbo:Infrastructure': 23, 'dbo:Hospital': 24}, {'dbo:Location': 0, 'dbo:Place': 1, 'dbo:Agent': 2, 'dbo:Settlement': 3, 'dbo:Person': 4, 'dbo:Species': 5, 'dbo:Event': 6, 'dbo:ArchitecturalStructure': 7, 'dbo:Insect': 8, 'dbo:PopulatedPlace': 9, 'dbo:Work': 10, 'dbo:Infrastructure': 11, 'dbo:Eukaryote': 12, 'dbo:Country': 13, 'dbo:Building': 14}, {'dbo:Location': 0, 'dbo:PopulatedPlace': 1, 'dbo:Agent': 2, 'dbo:Place': 3, 'dbo:Eukaryote': 4, 'dbo:ArchitecturalStructure': 5, 'dbo:Species': 6, 'dbo:Continent': 7}, {'dbo:Place': 0, 'dbo:Location': 1, 'dbo:Species': 2, 'dbo:PopulatedPlace': 3}, {'dbo:Location': 0, 'dbo:Place': 1}, {'dbo:Location': 0}])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PorG_IvOK4_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edda859b-8bf5-4840-c681-b2dd19775955"
      },
      "source": [
        "resource_models = []\n",
        "\n",
        "for l in range(1, 10):\n",
        "  # model for resource classification\n",
        "  # get which rows are for resource only\n",
        "  # must only include rows that have the type at the level \n",
        "  # of classification \n",
        "  train_resource_rows = ((df_train[\"category\"] == \"resource\") & (df_train[\"type\"].map(lambda x: len(x)) >= l)).values\n",
        "  y_train_type = etl.type_to_int(df_train[train_resource_rows], type_no=l)\n",
        "\n",
        "  clf_type = MLPClassifier(\n",
        "      random_state=seed, max_iter=10, hidden_layer_sizes=(1000, 500, 300), verbose = 2).\\\n",
        "    fit(X_train_category[train_resource_rows], y_train_type)\n",
        "\n",
        "  resource_models.append(clf_type)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.75082653\n",
            "Iteration 2, loss = 2.18218086\n",
            "Iteration 3, loss = 1.24369407\n",
            "Iteration 4, loss = 0.64103863\n",
            "Iteration 5, loss = 0.33665893\n",
            "Iteration 6, loss = 0.18465704\n",
            "Iteration 7, loss = 0.11803178\n",
            "Iteration 8, loss = 0.08212401\n",
            "Iteration 9, loss = 0.05672128\n",
            "Iteration 10, loss = 0.04667517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.08176374\n",
            "Iteration 2, loss = 1.61502754\n",
            "Iteration 3, loss = 0.83860591\n",
            "Iteration 4, loss = 0.41191638\n",
            "Iteration 5, loss = 0.21624394\n",
            "Iteration 6, loss = 0.12547668\n",
            "Iteration 7, loss = 0.08233299\n",
            "Iteration 8, loss = 0.06213343\n",
            "Iteration 9, loss = 0.04921928\n",
            "Iteration 10, loss = 0.04056298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.65745794\n",
            "Iteration 2, loss = 1.38340688\n",
            "Iteration 3, loss = 0.78548164\n",
            "Iteration 4, loss = 0.43509259\n",
            "Iteration 5, loss = 0.25352839\n",
            "Iteration 6, loss = 0.15392936\n",
            "Iteration 7, loss = 0.09815556\n",
            "Iteration 8, loss = 0.06791200\n",
            "Iteration 9, loss = 0.04692353\n",
            "Iteration 10, loss = 0.03643403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.10595446\n",
            "Iteration 2, loss = 1.13959116\n",
            "Iteration 3, loss = 0.65575704\n",
            "Iteration 4, loss = 0.38037346\n",
            "Iteration 5, loss = 0.21035060\n",
            "Iteration 6, loss = 0.11851739\n",
            "Iteration 7, loss = 0.07134551\n",
            "Iteration 8, loss = 0.04483559\n",
            "Iteration 9, loss = 0.03386175\n",
            "Iteration 10, loss = 0.02856816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.67182778\n",
            "Iteration 2, loss = 0.75242790\n",
            "Iteration 3, loss = 0.50482583\n",
            "Iteration 4, loss = 0.32265987\n",
            "Iteration 5, loss = 0.19467074\n",
            "Iteration 6, loss = 0.12864201\n",
            "Iteration 7, loss = 0.09078525\n",
            "Iteration 8, loss = 0.06691898\n",
            "Iteration 9, loss = 0.05017320\n",
            "Iteration 10, loss = 0.03608343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.84355178\n",
            "Iteration 2, loss = 1.02404255\n",
            "Iteration 3, loss = 0.44421446\n",
            "Iteration 4, loss = 0.36563381\n",
            "Iteration 5, loss = 0.33849055\n",
            "Iteration 6, loss = 0.27359887\n",
            "Iteration 7, loss = 0.22378917\n",
            "Iteration 8, loss = 0.17534064\n",
            "Iteration 9, loss = 0.14461985\n",
            "Iteration 10, loss = 0.12811526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.41588529\n",
            "Iteration 2, loss = 1.21247183\n",
            "Iteration 3, loss = 1.03879153\n",
            "Iteration 4, loss = 0.86206991\n",
            "Iteration 5, loss = 0.67719225\n",
            "Iteration 6, loss = 0.49752180\n",
            "Iteration 7, loss = 0.34289716\n",
            "Iteration 8, loss = 0.22089027\n",
            "Iteration 9, loss = 0.13430639\n",
            "Iteration 10, loss = 0.07825502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.70819303\n",
            "Iteration 2, loss = 0.59438630\n",
            "Iteration 3, loss = 0.50734746\n",
            "Iteration 4, loss = 0.41817254\n",
            "Iteration 5, loss = 0.32701643\n",
            "Iteration 6, loss = 0.24178263\n",
            "Iteration 7, loss = 0.16966726\n",
            "Iteration 8, loss = 0.11351057\n",
            "Iteration 9, loss = 0.07384308\n",
            "Iteration 10, loss = 0.04797684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.80342569\n",
            "Iteration 2, loss = 0.63708355\n",
            "Iteration 3, loss = 0.52221296\n",
            "Iteration 4, loss = 0.40919535\n",
            "Iteration 5, loss = 0.30137465\n",
            "Iteration 6, loss = 0.21079080\n",
            "Iteration 7, loss = 0.14513293\n",
            "Iteration 8, loss = 0.10457916\n",
            "Iteration 9, loss = 0.08208650\n",
            "Iteration 10, loss = 0.06973224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTmsnFgOy5Jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29a02c1-050f-436a-dba0-074e8aed38c0"
      },
      "source": [
        "clf_type.predict(X_train_category).shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17482,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4qZoWbK7KeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973c89af-bd02-4a98-e932-0cbdd083cfa9"
      },
      "source": [
        "clf_type.predict(X_train_category[train_resource_rows])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kib4GVC77UcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4840f787-a679-413a-96b1-9992ccecd388"
      },
      "source": [
        "clf_type.predict_proba"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method MLPClassifier.predict_proba of MLPClassifier(hidden_layer_sizes=(1000, 500, 300), max_iter=10,\n",
              "              random_state=20211001, verbose=2)>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHcB2B2n-Jc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af9a9a0-de59-41bf-ee9d-cf668da2ee55"
      },
      "source": [
        "clf_type.predict_proba(X_train_category[train_resource_rows]).shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fxgGgeOMM9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb50697e-b062-4d0a-f5fc-f8e5b6e3e475"
      },
      "source": [
        "clf_type.score(X_train_category[train_resource_rows], y_train_type) "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwcZ38iu_YYH"
      },
      "source": [
        "## 4. Save models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJnfBXgkfyUr"
      },
      "source": [
        "with open(os.path.join(\"sklearn_objects\", \"category_model.pkl\"), \"wb\") as mdl_file:\n",
        "  pickle.dump(clf_category, mdl_file)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_QaOf94RtQs"
      },
      "source": [
        "with open(os.path.join(\"sklearn_objects\", \"literal_model.pkl\"), \"wb\") as mdl_file:\n",
        "  pickle.dump(clf_literal, mdl_file)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASFCv0nkaphR"
      },
      "source": [
        "for l in range(1,10):\n",
        "  with open(os.path.join(\"sklearn_objects\", f\"resource_level_{l}_model.pkl\"), \"wb\") as mdl_file:\n",
        "    pickle.dump(resource_models[l-1], mdl_file)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pol2B42Cw2ck"
      },
      "source": [
        "## 5. Results & Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV76HT3oUoxa"
      },
      "source": [
        "***Load Pre-trained models***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmutptNCTqbc"
      },
      "source": [
        "with open(\"sklearn_objects/category_model.pkl\", \"rb\") as clf_cat_file:\n",
        "  clf_category = pickle.load(clf_cat_file)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxMJq2xsBOwL"
      },
      "source": [
        "with open(\"sklearn_objects/literal_model.pkl\", \"rb\") as clf_lit_file:\n",
        "  clf_literal = pickle.load(clf_lit_file)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jx_Y4PaBQ0q"
      },
      "source": [
        "resource_models = []\n",
        "for l in range(1,10):\n",
        "  with open(f\"sklearn_objects/resource_level_{l}_model.pkl\", \"rb\") as res_mdl:\n",
        "    resource_models.append(pickle.load(res_mdl))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP9mCDz0BTeD"
      },
      "source": [
        "***Load processing class***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYLTHE0gUll3"
      },
      "source": [
        "etl = ETL(path_to_type_maps=\"resource_types\", path_to_vectorizers=\"sklearn_objects\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orIFdQaDN9ba"
      },
      "source": [
        "class ModelEvaluation:\n",
        "  def __init__(self, etl_inst, cat_model, lit_model, res_models):\n",
        "    self.etl_inst = etl_inst\n",
        "    self.cat_model = cat_model\n",
        "    self.lit_model = lit_model\n",
        "    self.res_models = res_models\n",
        "  \n",
        "  # X is a df\n",
        "  def get_predictions(self, X, bow_type = \"tf\"):\n",
        "\n",
        "    X = X.copy()\n",
        "\n",
        "    X.reset_index(inplace = True, drop = True)\n",
        "    \n",
        "    X_norm = self.etl_inst.norm_data(X)\n",
        "    X_vec = self.etl_inst.bow_transform(X_norm, type = bow_type)\n",
        "\n",
        "    bool_int = self.etl_inst.category_map[\"boolean\"]\n",
        "    literal_int = self.etl_inst.category_map[\"literal\"]\n",
        "    resource_int = self.etl_inst.category_map[\"resource\"]\n",
        "\n",
        "    cat_pred = self.cat_model.predict(X_vec)\n",
        "\n",
        "    ind_bool = cat_pred == bool_int\n",
        "    ind_literal = cat_pred == literal_int\n",
        "    ind_resource = cat_pred == resource_int\n",
        "\n",
        "    if len(ind_bool) > 0:\n",
        "      X.loc[ind_bool, \"cat_prediction\"] = \"boolean\"\n",
        "      X.loc[ind_bool, \"type_prediction\"] = pd.Series(\n",
        "          cat_pred[ind_bool], name = \"type_prediction\")\\\n",
        "          .map(lambda x: [\"boolean\"]).values\n",
        "\n",
        "    if len(ind_literal) > 0:\n",
        "      X.loc[ind_literal, \"cat_prediction\"] = \"literal\"\n",
        "      literal_pred = self.lit_model.predict(X_vec[ind_literal])\n",
        "      X.loc[ind_literal, \"type_prediction\"] = pd.Series(\n",
        "          literal_pred, name = \"type_prediction\")\\\n",
        "          .map(lambda x: [self.etl_inst.inv_literal_map[x]]).values\n",
        "\n",
        "    if len(ind_resource) > 0:\n",
        "      resource_preds = []\n",
        "      for ind, type_model in enumerate(self.res_models):\n",
        "          resource_preds.append(\n",
        "            pd.Series(\n",
        "                type_model.predict(X_vec[ind_resource]), name = f\"type_{ind}\").\\\n",
        "                map(lambda x: self.etl_inst.invtype_maps[f\"type{ind+1}\"][x])\n",
        "                )\n",
        "      resource_preds = pd.Series(pd.concat(resource_preds, axis = 1).values.tolist(), name = \"type_prediction\")\n",
        "      X.loc[ind_resource, \"type_prediction\"] = resource_preds.values\n",
        "      X.loc[ind_resource, \"cat_prediction\"] = \"resource\"\n",
        "      \n",
        "      return X\n",
        "\n",
        "  def output_predictions(self):\n",
        "    return NotImplementedError"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6BLutJ8jA-M"
      },
      "source": [
        "me = ModelEvaluation(etl, clf_category, clf_literal, resource_models)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b_rDe2EKOry"
      },
      "source": [
        "***Validate***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00lDYJIE0pit"
      },
      "source": [
        "# out_val = me.get_predictions(df_val)\n",
        "\n",
        "# true_output = out_val.loc[:, [\"id\", \"question\", \"category\", \"type\"]]\n",
        "# true_output_dict = [pred for ind, pred in true_output.to_dict(orient = \"index\").items()]\n",
        "\n",
        "# system_output = out_val.loc[:, [\"id\", \"cat_prediction\", \"type_prediction\"]]\n",
        "# system_output.columns = [\"id\", \"category\", \"type\"]\n",
        "# system_output_dict = [pred for ind, pred in system_output.to_dict(orient = \"index\").items()]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Test***"
      ],
      "metadata": {
        "id": "ZfzCuXlfbpTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_test = me.get_predictions(test_data2020)\n",
        "\n",
        "true_output = test_data2020.loc[:, [\"id\", \"question\", \"category\", \"type\"]]\n",
        "true_output_dict = [pred for ind, pred in true_output.to_dict(orient = \"index\").items()]\n",
        "\n",
        "system_output = out_test.loc[:, [\"id\", \"cat_prediction\", \"type_prediction\"]]\n",
        "system_output.columns = [\"id\", \"category\", \"type\"]\n",
        "system_output_dict = [pred for ind, pred in system_output.to_dict(orient = \"index\").items()]"
      ],
      "metadata": {
        "id": "hHa-NgNLbd8P"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Run Evaluation***"
      ],
      "metadata": {
        "id": "AgOsh-MWbmjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"system_output/\", exist_ok = True)\n",
        "with open(os.path.join(\"system_output\", \"ground_truth_json.json\"), \"w\") as gfile:\n",
        "  json.dump(true_output_dict, gfile)\n",
        "\n",
        "with open(os.path.join(\"system_output\", \"system_output_json.json\"), \"w\") as sfile:\n",
        "  json.dump(system_output_dict, sfile)"
      ],
      "metadata": {
        "id": "1P7f1sa4beOu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py --type_hierarchy_tsv dbpedia_types.tsv  \\\n",
        " --ground_truth_json system_output/ground_truth_json.json \\\n",
        " --system_output_json system_output/system_output_json.json"
      ],
      "metadata": {
        "id": "ps9toD54cAIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19cd9b61-18b4-4670-d287-fda24c4d444b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading type hierarchy from dbpedia_types.tsv... 761 types loaded (max depth: 7)\n",
            "Loading ground truth from system_output/ground_truth_json.json... \n",
            "   4366 questions loaded\n",
            "Loading system predictions from system_output/system_output_json.json... \n",
            "   4366 predictions loaded\n",
            "\n",
            "\n",
            "Evaluation results:\n",
            "-------------------\n",
            "Category prediction (based on 4366 questions)\n",
            "  Accuracy: 0.939\n",
            "Type ranking (based on 4366 questions)\n",
            "  NDCG@5:  0.724\n",
            "  NDCG@10: 0.687\n"
          ]
        }
      ]
    }
  ]
}